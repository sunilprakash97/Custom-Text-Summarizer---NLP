{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Libraries"
      ],
      "metadata": {
        "id": "-pJJrAFF6Ri7"
      },
      "id": "-pJJrAFF6Ri7"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4ce04402",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ce04402",
        "outputId": "ceb175cb-aa9a-42c1-c0b0-f1a92cf07d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re                        \n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statistics import mode\n",
        "from bs4 import BeautifulSoup  \n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import LancasterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import backend as K \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,Attention\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/Reviews.csv',nrows=100000)\n",
        "\n",
        "# data = pd.read_csv(\"Reviews.csv\",nrows=100000)\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDlhfUe2ntKw",
        "outputId": "2c26c9ff-b7cf-4e2f-9a07-06bd58099cc6"
      },
      "id": "SDlhfUe2ntKw",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count   Dtype \n",
            "---  ------                  --------------   ----- \n",
            " 0   Id                      100000 non-null  int64 \n",
            " 1   ProductId               100000 non-null  object\n",
            " 2   UserId                  100000 non-null  object\n",
            " 3   ProfileName             99996 non-null   object\n",
            " 4   HelpfulnessNumerator    100000 non-null  int64 \n",
            " 5   HelpfulnessDenominator  100000 non-null  int64 \n",
            " 6   Score                   100000 non-null  int64 \n",
            " 7   Time                    100000 non-null  int64 \n",
            " 8   Summary                 99998 non-null   object\n",
            " 9   Text                    100000 non-null  object\n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 7.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the Data"
      ],
      "metadata": {
        "id": "bZOmVHMS6Y8R"
      },
      "id": "bZOmVHMS6Y8R"
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "jbRH_OZBTNJW"
      },
      "id": "jbRH_OZBTNJW",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the Data"
      ],
      "metadata": {
        "id": "ulZYirla6eNz"
      },
      "id": "ulZYirla6eNz"
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
        "data.dropna(axis = 0, inplace = True)\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    \n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping \n",
        "                          else t for t in newString.split(\" \")])    \n",
        "\n",
        "    if(num == 0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens = newString.split()\n",
        "\n",
        "    long_words = []\n",
        "    for i in tokens:\n",
        "        if len(i) > 3:                                           \n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t,0)) \n",
        "\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))\n",
        "\n",
        "data['cleaned_text'] = cleaned_text\n",
        "data['cleaned_summary'] = cleaned_summary\n",
        "\n",
        "data.replace('', np.nan, inplace = True)\n",
        "data.dropna(axis = 0, inplace = True)\n",
        "\n",
        "max_text_len = 30\n",
        "max_summary_len = 8\n",
        "\n",
        "cleaned_text = np.array(data['cleaned_text'])\n",
        "cleaned_summary = np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split()) <= max_summary_len and \n",
        "       len(cleaned_text[i].split()) <= max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "data = pd.DataFrame({'Text':short_text,'Summary':short_summary})\n",
        "data.info()"
      ],
      "metadata": {
        "id": "uO6OdJ0rTQYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd3a525-66eb-43b6-b833-fce437b9a90c"
      },
      "id": "uO6OdJ0rTQYU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 52559 entries, 0 to 52558\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Text     52559 non-null  object\n",
            " 1   Summary  52559 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 821.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalizing the Cleaned Data"
      ],
      "metadata": {
        "id": "ebJeVa446iIR"
      },
      "id": "ebJeVa446iIR"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fb39122c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb39122c",
        "outputId": "b873b081-751b-4d5e-bd65-019d1f918f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of input words :  16597\n",
            "number of target words :  9714\n",
            "maximum input length :  72\n",
            "maximum target length :  19\n"
          ]
        }
      ],
      "source": [
        "data.drop_duplicates(subset = ['Text'], inplace = True)\n",
        "data.dropna(axis = 0, inplace = True)\n",
        "input_data = data.loc[:,'Text']\n",
        "target_data = data.loc[:,'Summary']\n",
        "target_data.replace('', np.nan, inplace = True)\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_words = []\n",
        "target_words = []\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemm = LancasterStemmer()\n",
        "\n",
        "# This function is to clean the User's Input text\n",
        "def clean(texts,src):\n",
        "  words = word_tokenize(texts.lower())\n",
        "  words = list(filter(lambda w:(w.isalpha() and len(w) >= 3),words))\n",
        "  \n",
        "  # Stem the words to their root word and filter stop words\n",
        "  if src == \"inputs\":\n",
        "    words = [stemm.stem(w) for w in words if w not in stop_words]\n",
        "  else:\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "  return words\n",
        "\n",
        "for in_txt, tr_txt in zip(input_data, target_data):\n",
        "  in_words = clean(in_txt,\"inputs\")\n",
        "  input_texts += [' '.join(in_words)]\n",
        "  input_words += in_words\n",
        "\n",
        "  tr_words = clean(\"sos \"+ tr_txt +\" eos\", \"target\")\n",
        "  target_texts += [' '.join(tr_words)]\n",
        "  target_words += tr_words\n",
        "\n",
        "# Store only unique words\n",
        "input_words = sorted(list(set(input_words)))\n",
        "target_words = sorted(list(set(target_words)))\n",
        "num_in_words = len(input_words) \n",
        "num_tr_words = len(target_words)\n",
        "\n",
        "# Counting the Input and target texts which appears most often  \n",
        "max_in_len = mode([len(i) for i in input_texts])\n",
        "max_tr_len = mode([len(i) for i in target_texts])\n",
        "\n",
        "print(\"number of input words : \",num_in_words)\n",
        "print(\"number of target words : \",num_tr_words)\n",
        "print(\"maximum input length : \",max_in_len)\n",
        "print(\"maximum target length : \",max_tr_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting into Training Dataset and Tokenizing the Data"
      ],
      "metadata": {
        "id": "BCroKrIr6pBC"
      },
      "id": "BCroKrIr6pBC"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0f4a6af1",
      "metadata": {
        "id": "0f4a6af1"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input_texts,\n",
        "                                                    target_texts,\n",
        "                                                    test_size = 0.1,\n",
        "                                                    random_state = 1)\n",
        "\n",
        "#train the tokenizer with all the words\n",
        "in_tokenizer = Tokenizer()\n",
        "in_tokenizer.fit_on_texts(x_train)\n",
        "tr_tokenizer = Tokenizer()\n",
        "tr_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "#convert text into sequence of integers\n",
        "x_train = in_tokenizer.texts_to_sequences(x_train) \n",
        "y_train = tr_tokenizer.texts_to_sequences(y_train) \n",
        "\n",
        "#padding the array\n",
        "en_in_data = pad_sequences(x_train,  maxlen = max_in_len, padding='post') \n",
        "dec_data = pad_sequences(y_train,  maxlen = max_tr_len, padding='post')\n",
        "\n",
        "#decoder input will excludes the last word i.e. 'eos'\n",
        "dec_in_data = dec_data[:,:-1]\n",
        "#decoder target excludes the first word i.e 'sos'\n",
        "dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Model"
      ],
      "metadata": {
        "id": "Qfk4UPOl6_9x"
      },
      "id": "Qfk4UPOl6_9x"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e7e2469d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7e2469d",
        "outputId": "55fa164d-3201-4cd5-f5cd-1f969d652ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 72)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 72, 300)      4979400     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 72, 300),    721200      ['embedding[0][0]']              \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 72, 300),    721200      ['lstm[0][0]']                   \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 300)    2914500     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 72, 300),    721200      ['lstm_1[0][0]']                 \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 300),  721200      ['embedding_1[0][0]',            \n",
            "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " attention (Attention)          (None, None, 300)    0           ['lstm_3[0][0]',                 \n",
            "                                                                  'lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            " concat_layer1 (Concatenate)    (None, None, 600)    0           ['lstm_3[0][0]',                 \n",
            "                                                                  'attention[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 9715)   5838715     ['concat_layer1[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16,617,415\n",
            "Trainable params: 16,617,415\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "K.clear_session() \n",
        "latent_dim = 300\n",
        "en_inputs = Input(shape = (max_in_len,)) \n",
        "en_embedding = Embedding(num_in_words+1, latent_dim)(en_inputs) \n",
        "\n",
        "# LSTM 1\n",
        "en_lstm1 = LSTM(latent_dim, \n",
        "                return_state = True, \n",
        "                return_sequences = True, \n",
        "                dropout = 0.4, \n",
        "                recurrent_dropout = 0.4) \n",
        "en_outputs1, state_h1, state_c1 = en_lstm1(en_embedding) \n",
        "\n",
        "# LSTM2\n",
        "en_lstm2 = LSTM(latent_dim,\n",
        "                return_state = True,\n",
        "                return_sequences = True, \n",
        "                dropout = 0.4, \n",
        "                recurrent_dropout = 0.4)\n",
        "en_outputs2, state_h2, state_c2 = en_lstm2(en_outputs1) \n",
        "\n",
        "# LSTM3\n",
        "en_lstm3 = LSTM(latent_dim, \n",
        "                return_state = True, \n",
        "                return_sequences = True, \n",
        "                dropout = 0.4, \n",
        "                recurrent_dropout = 0.4)\n",
        "en_outputs3 , state_h3, state_c3 = en_lstm3(en_outputs2)\n",
        "en_states = [state_h3, state_c3]\n",
        "\n",
        "# Decoder \n",
        "dec_inputs = Input(shape = (None, )) \n",
        "dec_emb_layer = Embedding(num_tr_words+1, latent_dim) \n",
        "dec_embedding = dec_emb_layer(dec_inputs) \n",
        "\n",
        "# Initialize decoder's LSTM\n",
        "dec_lstm = LSTM(latent_dim, return_sequences = True, \n",
        "                return_state = True, dropout = 0.4, recurrent_dropout = 0.4)\n",
        "dec_outputs, *_ = dec_lstm(dec_embedding, initial_state = en_states) \n",
        "\n",
        "# Attention layer\n",
        "attention = Attention()\n",
        "attn_out = attention([dec_outputs, en_outputs3])\n",
        "\n",
        "# Concatenate the attention output with the decoder ouputs\n",
        "merge = Concatenate(axis = -1, name = 'concat_layer1')([dec_outputs, attn_out])\n",
        "\n",
        "# Dense layer (output layer)\n",
        "dec_dense = Dense(num_tr_words+1, activation = 'softmax') \n",
        "dec_outputs = dec_dense(merge) \n",
        "\n",
        "model = Model([en_inputs, dec_inputs], dec_outputs) \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "390e14e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "390e14e7",
        "outputId": "89946adc-9fc3-45ca-b9bb-68a27e6f5344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "84/84 [==============================] - 110s 1s/step - loss: 1.3111 - accuracy: 0.8399 - val_loss: 0.9572 - val_accuracy: 0.8766\n",
            "Epoch 2/50\n",
            "84/84 [==============================] - 93s 1s/step - loss: 0.9366 - accuracy: 0.8764 - val_loss: 0.9392 - val_accuracy: 0.8769\n",
            "Epoch 3/50\n",
            "84/84 [==============================] - 102s 1s/step - loss: 0.9102 - accuracy: 0.8770 - val_loss: 0.9162 - val_accuracy: 0.8776\n",
            "Epoch 4/50\n",
            "84/84 [==============================] - 104s 1s/step - loss: 0.8823 - accuracy: 0.8780 - val_loss: 0.8968 - val_accuracy: 0.8775\n",
            "Epoch 5/50\n",
            "84/84 [==============================] - 94s 1s/step - loss: 0.8528 - accuracy: 0.8787 - val_loss: 0.8759 - val_accuracy: 0.8788\n",
            "Epoch 6/50\n",
            "84/84 [==============================] - 93s 1s/step - loss: 0.8339 - accuracy: 0.8790 - val_loss: 0.8613 - val_accuracy: 0.8791\n",
            "Epoch 7/50\n",
            "84/84 [==============================] - 93s 1s/step - loss: 0.8128 - accuracy: 0.8798 - val_loss: 0.8552 - val_accuracy: 0.8793\n",
            "Epoch 8/50\n",
            "84/84 [==============================] - 93s 1s/step - loss: 0.7959 - accuracy: 0.8806 - val_loss: 0.8560 - val_accuracy: 0.8793\n",
            "Epoch 9/50\n",
            "84/84 [==============================] - 93s 1s/step - loss: 0.7794 - accuracy: 0.8813 - val_loss: 0.8562 - val_accuracy: 0.8801\n",
            "Epoch 9: early stopping\n",
            "INFO:tensorflow:Assets written to: s2s/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: s2s/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f545a6de150> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f545a7c6710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f545b569b10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f545bb31650> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ],
      "source": [
        "plot_model(model, to_file ='model_plot.png', show_shapes = True, show_layer_names = True)\n",
        "\n",
        "model.compile(optimizer = \"rmsprop\", \n",
        "              loss = \"sparse_categorical_crossentropy\", \n",
        "              metrics = [\"accuracy\"] ) \n",
        "\n",
        "es = EarlyStopping(monitor = 'val_loss', \n",
        "                   mode ='min', \n",
        "                   verbose = 1, \n",
        "                   patience = 2)\n",
        "\n",
        "graph = model.fit([en_in_data, dec_in_data],\n",
        "                    dec_tr_data, \n",
        "                    batch_size = 512, \n",
        "                    epochs = 50,\n",
        "                    callbacks = [es], \n",
        "                    validation_split = 0.1)\n",
        "\n",
        "model.save(\"Model\")\n",
        "# encoder inference\n",
        "latent_dim = 300\n",
        "model = models.load_model(\"Model\")\n",
        "\n",
        "# encoder model\n",
        "en_outputs, state_h_enc, state_c_enc = model.layers[6].output\n",
        "en_states = [state_h_enc, state_c_enc]\n",
        "en_model = Model(model.input[0], [en_outputs] + en_states)\n",
        "\n",
        "# decoder inference\n",
        "dec_state_input_h = Input(shape = (latent_dim, ))\n",
        "dec_state_input_c = Input(shape = (latent_dim, ))\n",
        "dec_hidden_state_input = Input(shape = (max_in_len, latent_dim))\n",
        "\n",
        "# Get the embeddings and input layer from the model\n",
        "dec_inputs = model.input[1]\n",
        "dec_emb_layer = model.layers[5]\n",
        "dec_lstm = model.layers[7]\n",
        "dec_embedding = dec_emb_layer(dec_inputs)\n",
        "\n",
        "# add input and initialize LSTM layer with encoder LSTM states.\n",
        "dec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, \n",
        "                                            initial_state = [dec_state_input_h,\n",
        "                                                             dec_state_input_c])\n",
        "\n",
        "#Attention layer\n",
        "attention = model.layers[8]\n",
        "attn_out2 = attention([dec_outputs2, dec_hidden_state_input])\n",
        "\n",
        "merge2 = Concatenate(axis = -1)([dec_outputs2, attn_out2])\n",
        "\n",
        "#Dense layer\n",
        "dec_dense = model.layers[10]\n",
        "dec_outputs2 = dec_dense(merge2)\n",
        "\n",
        "# Finally define the Model Class\n",
        "dec_model = Model(\n",
        "[dec_inputs] + [dec_hidden_state_input, dec_state_input_h, dec_state_input_c],\n",
        "[dec_outputs2] + [state_h2, state_c2])\n",
        "\n",
        "# creating a dictionary with a key as index and value as words.\n",
        "reverse_target_word_index = tr_tokenizer.index_word\n",
        "reverse_source_word_index = in_tokenizer.index_word\n",
        "target_word_index = tr_tokenizer.word_index\n",
        "reverse_target_word_index[0]=' '\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    en_out, en_h, en_c= en_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_word_index['sos']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    \n",
        "    while not stop_condition: \n",
        "        output_words, dec_h, dec_c = dec_model.predict([target_seq] + [en_out,\n",
        "                                                                       en_h, \n",
        "                                                                       en_c])\n",
        "        word_index = np.argmax(output_words[0, -1, :])\n",
        "        text_word = reverse_target_word_index[word_index]\n",
        "        decoded_sentence += text_word +\" \"\n",
        "        if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n",
        "          stop_condition = True\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = word_index\n",
        "        en_h, en_c = dec_h, dec_c\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(graph.history['loss'], label='train')\n",
        "pyplot.plot(graph.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "WeyiXhU-C1CE",
        "outputId": "fe64dc1e-3c81-4c09-8a92-ac5337f84e44"
      },
      "id": "WeyiXhU-C1CE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3Tcd3nn8feju3W/zMiyJNuS7ZHvsWM798iJGwJOoAHKHtqwoWe7hfBH2y3dJUvYw6W021N62uUA2wNsCG7oUtJNEzhlSVgM1MZ27rKT+G5dfIkl2bpaN1uSdfnuH7+RNFIsyZZHmtvndY6ONPP7zcyjYD7++pnv7xlzziEiIrEvKdIFiIhIeCjQRUTihAJdRCROKNBFROKEAl1EJE6kROqFfT6fq6ioiNTLi4jEpIMHD7Y75/zXOhaxQK+oqKCmpiZSLy8iEpPM7Nx0x9RyERGJEwp0EZE4oUAXEYkTEeuhi4jMxdDQEI2NjQwMDES6lHmVkZFBeXk5qamp1/0YBbqIxJTGxkZycnKoqKjAzCJdzrxwztHR0UFjYyOVlZXX/Ti1XEQkpgwMDFBUVBS3YQ5gZhQVFd3wv0IU6CISc+I5zMfM5XeMuUCvbenlv//sOANDI5EuRUQkqsRcoDdeusLTB85Qc/ZSpEsRkQTU1dXFt7/97Rt+3MMPP0xXV9c8VDRh1kA3s11m1mpmR6c5/mEzO2xmb5tZjZndG/4yJ9xRWURqsrG/rm0+X0ZE5JqmC/Th4eEZH/fSSy+Rn58/X2UB17dCfwbYOcPxXwObnHObgf8IPB2GuqaVlZ7C1uUF7Ktrn8+XERG5pieffJKGhgY2b97MbbfdRnV1NY888gjr1q0D4CMf+Qhbt25l/fr1PPXUU+OPq6iooL29nbNnz7J27Vo+/elPs379et7//vfT398fltpm3bbonNtnZhUzHO8LuZkFzPtn2lUH/PztL07R2jtAcU7GfL+ciESpr/7fYxxv7gnrc64rzeUrv71+2uNf+9rXOHr0KG+//TZ79+7lgx/8IEePHh3fXrhr1y4KCwvp7+/ntttu42Mf+xhFRUWTnqOuro5nn32W733ve3z84x/nhRde4LHHHrvp2sPSQzezj5rZSeBFvFX6dOc9HmzL1LS1zb1lsj3gDRp7uV6rdBGJrNtvv33SXvFvfetbbNq0iTvvvJPz589TV1f3nsdUVlayefNmALZu3crZs2fDUktYLixyzv0E+ImZbQf+EnjfNOc9BTwFsG3btjmv5NeX5lKQmcr+unY+emv5XJ9GRGLcTCvphZKVlTX+8969e/nVr37Fq6++SmZmJvfff/8195Knp6eP/5ycnBy2lktYd7k45/YBK8zMF87nnSopybg34Gd/XTvOzXuHR0RkXE5ODr29vdc81t3dTUFBAZmZmZw8eZLXXnttQWu76UA3s1UW3AFvZluAdKDjZp93NtUBH229g5xqufZ/WBGR+VBUVMQ999zDhg0beOKJJyYd27lzJ8PDw6xdu5Ynn3ySO++8c0Frm7XlYmbPAvcDPjNrBL4CpAI4574LfAz4fTMbAvqB33ULsGyuDnj/CNhf286aktz5fjkRkXE/+tGPrnl/eno6P//5z695bKxP7vP5OHp0Yhf45z73ubDVdT27XB6d5fjfAH8Ttoqu05K8RQSKs9lX18ant69Y6JcXEYk6MXelaKjqgJ83znRqDICICDEf6D4Gh0d582xnpEsREYm4mA70O1YUBscAaD+6iEhMB3pmWgrblheyr1ZzXUREYjrQAaqrfJy82EtrT3x/HJWIyGxiPtDHxgAc0BgAEVkAcx2fC/CNb3yDK1euhLmiCTEf6OuW5FKYlaY+uogsiGgO9Jj/kOikJOPeVT7217UzOupISor/j6YSkcgJHZ/74IMPUlxczHPPPcfg4CAf/ehH+epXv8rly5f5+Mc/TmNjIyMjI3zpS1+ipaWF5uZmduzYgc/nY8+ePWGvLeYDHbztiz99p5mTF3tZV6qrRkUSxs+fhItHwvucJRvhoa9Nezh0fO7u3bt5/vnneeONN3DO8cgjj7Bv3z7a2tooLS3lxRdfBLwZL3l5eXz9619nz549+HzzM+4q5lsu4F1gBOhTjERkQe3evZvdu3dz6623smXLFk6ePEldXR0bN27kl7/8JZ///OfZv38/eXl5C1JPXKzQS/IyqFqczf66dj5z38pIlyMiC2WGlfRCcM7xhS98gc985jPvOXbo0CFeeuklvvjFL/LAAw/w5S9/ed7riYsVOgTHAJztpP+qxgCIyPwJHZ/7gQ98gF27dtHX531wW1NTE62trTQ3N5OZmcljjz3GE088waFDh97z2PkQFyt08Pro3z9whjfOdnJflT/S5YhInAodn/vQQw/xiU98grvuuguA7OxsfvjDH1JfX88TTzxBUlISqampfOc73wHg8ccfZ+fOnZSWls7Lm6IWqQ+I2LZtm6upqQnb8/VfHWHTV3fz+3ct54sfWhe25xWR6HLixAnWrl0b6TIWxLV+VzM76Jzbdq3z46blsigtmdsqC3SBkYgkrLgJdPD66BoDICKJKs4CPfgpRrpqVCSuJcJnCc/ld4yrQF9bkosvO0370UXiWEZGBh0dHXEd6s45Ojo6yMjIuKHHxc0uF/DGANyzyseBeo0BEIlX5eXlNDY20tYW3wu3jIwMysvLb+gxcRXo4PXR//XtZk5c7GF96cJcnSUiCyc1NZXKyspIlxGV4qrlAuqji0jiirtAX5ybwerFOeqji0jCibtAB2+V/uaZSxoDICIJJT4DvcrP1ZFRXj/TEelSREQWTFwG+u0VhaSlJKmPLiIJJS4DfVFaMrdXFKqPLiIJJS4DHbw+em1LHxe7NQZARBJDHAe6PsVIRBLLrIFuZrvMrNXMjk5z/N+b2WEzO2Jmr5jZpvCXeePWlOTgy05XH11EEsb1rNCfAXbOcPwMcJ9zbiPwl8BTYajrpiUlGdWBiTEAIiLxbtZAd87tAzpnOP6Kc+5S8OZrwI0NH5hH1QEfnZevcvxCT6RLERGZd+Huof8h8PMwP+ec3bvKGwOwT310EUkAYQt0M9uBF+ifn+Gcx82sxsxqFmJSWnFuBmtKcjigPrqIJICwBLqZ3QI8DXzYOTft5ZnOuaecc9ucc9v8/oX5IOftVX5qzl7iytXhBXk9EZFIuelAN7NlwI+BTzrnam++pPCqDviCYwCmfRtARCQuzDoP3cyeBe4HfGbWCHwFSAVwzn0X+DJQBHzbzACGp/tE6ki4raKQ9JQk9te2s2N1caTLERGZN7MGunPu0VmOfwr4VNgqCrOM1GRur9QYABGJf3F7pWio6oCPutY+LnT3R7oUEZF5kyCBPjYGQLtdRCR+JUSgawyAiCSChAh0M2N7wMeBujaNARCRuJUQgQ5QXeXj0pUhjjVrDICIxKeECfR7NAZAROJcwgR6cU4Ga5fkavuiiMSthAl0gO0BHwfPXeLyoMYAiEj8SahArw74GRpxvH5m2nEzIiIxK6ECfVtFAekpSeyr1fZFEYk/CRXoGanJ3LGiSH10EYlLCRXo4PXRG9ou09SlMQAiEl8SLtDHxgAc0CpdROJMwgV61eJsinPS2acxACISZxIu0M2M6oCfl+vbGdEYABGJIwkX6ADbq3x0XRniWHN3pEsREQmbhAz0sTEAmr4oIvEkIQPdl53OuiW57KvVG6MiEj8SMtDBm7546N1L9GkMgIjEiYQN9O1jYwBOawyAiMSHhA30rcsLyEhNUh9dROJGwgZ6Rmoyd1QWaT66iMSNhA10gOqAj9Ntl2m8dCXSpYiI3LSEDvTtVWNjANR2EZHYl9CBHijOZnFuuvroIhIXEjrQx8YAHNAYABGJAwkd6OD10bv7hzjSpDEAIhLbEj7Q7x0bA6CrRkUkxiV8oBdlp7OhLFd9dBGJebMGupntMrNWMzs6zfE1ZvaqmQ2a2efCX+L8qw74OfTuJXoHhiJdiojInF3PCv0ZYOcMxzuB/wT8XTgKioTqgI/hUcdrpzsjXYqIyJzNGujOuX14oT3d8Vbn3JtAzC5vty4vYFFqsj48WkRi2oL20M3scTOrMbOatrboCc/0lGTuXFGoC4xEJKYtaKA7555yzm1zzm3z+/0L+dKzujfg53T7Zc53agyAiMSmhN/lMmZ7wNu+eKBeq3QRiU0K9KBVxdmU5Gaojy4iMStlthPM7FngfsBnZo3AV4BUAOfcd82sBKgBcoFRM/sssM451zNvVc8DbwyAj18cu8jIqCM5ySJdkojIDZk10J1zj85y/CJQHraKIqi6ys+/HGzkcGMXty4riHQ5IiI3RC2XEPeu8mGGrhoVkZikQA9RmJXGhtI89dFFJCYp0KeoDvg49G6XxgCISMxRoE9RHfAzMup4taEj0qWIiNwQBfoUW5bnk5mWrD66iMQcBfoU3hiAIvXRRSTmKNCvoTrg42zHFd7t0BgAEYkdCvRrqA54c2b212uVLiKxQ4F+DSv9WZTmZbC/Vn10EYkdCvRr8MYA+Hm5oZ3hkdFIlyMicl0U6NOorvLROzDMO43dkS5FROS6KNCncc/KsTEA6qOLSGxQoE+jICuNW8rytB9dRGKGAn0G9wZ8vH2+ix6NARCRGKBAn4HGAIhILFGgz2DLsoLgGAD10UUk+inQZ5CWksRdK4rURxeRmKBAn0V1wMe5jiuc67gc6VJERGakQJ9FdVVwDIBW6SIS5RTos1jhy6Isf5H66CIS9RTos/DGAPh4pb5DYwBEJKop0K9DdcBP7+Aw7zR2RboUEZFpKdCvwz2rijCDfZq+KCJRTIF+HfIz07ilPF99dBGJagr067Q9OAagu19jAEQkOinQr1N1wM+og1cb1HYRkeikQL9Oty7LJystmX3ajy4iUUqBfp1Sk5O4a6WPfbVtOOciXY6IyHvMGuhmtsvMWs3s6DTHzcy+ZWb1ZnbYzLaEv8zosL3KR+Olfs51XIl0KSIi73E9K/RngJ0zHH8ICAS/Hge+c/NlRafqwNgYAO12EZHoM2ugO+f2AZ0znPJh4B+d5zUg38yWhKvAaFJRlElZ/iL10UUkKoWjh14GnA+53Ri87z3M7HEzqzGzmra22Fvlmhnbq3y81tDBkMYAiEiUWdA3RZ1zTznntjnntvn9/oV86bAZHwNwXmMARCS6hCPQm4ClIbfLg/fFpbtXFpFkqO0iIlEnHIH+U+D3g7td7gS6nXMXwvC8UUljAEQkWqXMdoKZPQvcD/jMrBH4CpAK4Jz7LvAS8DBQD1wB/mC+io0W2wM+/n5PPd1XhsjLTI10OSIiwHUEunPu0VmOO+CPwlZRDKiu8vOtf6vnlYZ2HtoYlxt6RCQG6UrROdi8NJ/s9BT10UUkqijQ58AbA1CkMQAiElUU6HO0PeCjqaufsxoDICJRQoE+RxoDICLRRoE+R8uLMllauEgfSyciUUOBPkdmRnXAz6sN7RoDICJRQYF+E7YHfFy+OsJb72oMgIhEngL9Jty10keSqY8uItEh9gJ9dBSiZKtg3qJUNi/N1350EYkKs14pGnXqfwU/+QwsXg/F62DxOiheD8VrIT17wcupDvj51r/V0XXlKvmZaQv++iIiY2Iv0LP9sPZD0HIc3vohDF2eOJa/fErQr4OiVZA8f/NWqgM+vvnrOl6u7+CDt2gMgIhETuwFeumt8Mj/9H4eHYWuc9B6AlqPeSHfehxqfwFuxDsnOQ18VZNX84vXQW4ZmN10OZuW5pOTnsL+ujYFuohEVOwFeqikJCis9L7WPDxx//AgtNcGAz4Y9OdehiPPTZyTkeeF/NS2zaL8GyphbAzA/rp2nHNYGP6SEBGZi9gO9OmkpEPJRu8rVP8lbzXfcsxbybcchyPPQ033xDm5Ze9dzfuqvOecRnWVn93HWzjTfpkV/oXv44uIQLwG+nQWFcDyu72vMc5BT9Pk1XzrcTi9F0aHvHOSUrxe/NSgz1sGSUlsD/gA2F/XrkAXkYhJrEC/FjPIK/e+qt4/cf/IEHTUT17NN9XAsR9PnJOWDf41LF+8jj/LSaP1cDNsfTQiu21ERCxS41+3bdvmampqIvLaN2Ww971tm9ZjXjsHcJaELV4PS++A8tth6e1QUBGWN2BFRMzsoHNu27WOaYV+o9JzvJBeevvEfc7xyjvHeOq5f2Vbcj3bO8+wpvVHpL35tHc8yx8M+Nu876WbIXVRZOoXkbilQA8HM+7evAHLWcJvatv4UkM7x5ousYpG7kit40F3jg3n3iH/5M+885NSYcktEyv4pbd7LR8RkZuglss86e4f4vXTHbzS0MGrDR2caumliG7uTj/Dw/nn2Gz1LO49StLIoPeA3LKJFfzSO7wdOim68lREJpup5aJAXyBtvYO8Nh7w7ZztuEIqw9yR2cyHixq5Lbmesr4jpPY1eQ9IyfAuohoP+dshuziyv4SIRJwCPQo1dfXzakMHrzS080p9Bxd7BgDYmHOZjxU3c3daAxX9x0hrPTyxfbKgYnKbpng9JKtrJpJIFOhRzjnH2Y4rXrgHWzSdl68CEChM4XeWtLN90RlWDR4n/UIN9LV4D0zNgrItE22a8m2QWRjB30RE5psCPcaMjjpqW3t5pd5r0bx+uoPewWEAqoqzeHjpEL+VfZbVV0+QfrEGLh6dmF3jq5q8ivet9kYkiEhcUKDHuOGRUY419/BKsEXz5tlOBoZGMYMNpXncV5HJ+/KaWDdygrQLB+H86+P74knP81buSzZ5O2tKboGCSoW8SIxSoMeZweER3jnfPd6ieevdSwyNOFKSjM1L87l7RSE7/L1sGD1JavOb0FgDbScnVvFpOVCyITjv5hYv6P1rZpxXIyLRQYEe565cHebguUu8XO/toDnS1M2og/SUJLZVFHD3Sh/VFdmsT71AcsthuHgELh72WjVj8+STUr1QL9k4sZIv2eBNpRSRqKFATzDd/UO8caaTVxraebWhg5MXewHIz0zlnlU+qlf5qK7yU5abBp1n4OI7XshfOOwF/eWQz0gtqAiG+y0TQZ9TolEGIhFy04FuZjuBbwLJwNPOua9NOb4c2AX4gU7gMedc40zPqUBfOG29g7zS0M6+2nb217XR2utdzLTCn8X2gJ/qgI87VhSRnZ7iTZ/sa5kI94uHvZ8vnZl4wkxfMNzHWjaboHAFJCVH6DcUSRw3FehmlgzUAg8CjcCbwKPOueMh5/wL8DPn3A/M7LeAP3DOfXKm51WgR4ZzjtqWPvbXtbG/rp3Xz3QwMDRKSpKxZXkB2wM+qgN+NpTlkZwUsgof6IGWo8GgP+Kt6ltPTuyRT83yPv4vNOiL10FqRmR+UZE4dbOBfhfw5865DwRvfwHAOffXIeccA3Y6586b95E93c653JmeV4EeHQaGRjh47hL767zV+7HmHiDYnlnpozoQbM/kX2OY2PBV783Wi4dDWjZH4KrX4sGSwb862LLZOBH2iwoW8DcUiS83G+j/Di+sPxW8/UngDufcH4ec8yPgdefcN83sd4AXAJ9zrmPKcz0OPA6wbNmyrefOnbuJX0vmQ3vfIC/Xe+2ZA/VttPRMbs/cu8rHnSuD7ZlrGR2FrrMhK/lgy6bv4sQ5ecsm+vGlm70RBxprIHJdFiLQS4G/ByqBfcDHgA3Oua7pnlcr9OjnnKOutY99tTfYnrmWvtYpK/nD0NEABP/85ZZPhPvYl656FXmPeW+5TDk/GzjpnJtxHqwCPfYMDI1w6Nwl9k1pz+QtSuXeVV575t6Aj/KCzOt7wsFeL9yb35r46myYOF5QERLwW7w3XzNm7OSJxL2bDfQUvDdFHwCa8N4U/YRz7ljIOT6g0zk3amZ/BYw457480/Mq0GPfWHtmrP8+3p7xZXm994B/5vbMtfR3wYW3J4d817sTx4sCEyFftsXryadlhfk3E4le4di2+DDwDbxti7ucc39lZn8B1Djnfhpsy/w13r+f9wF/5JwbnOk5FejxZaw9Mxbur52e3J4Z2/u+8XraM1Nd7pgc8M1vQW+zd8ySvAuiQlfyi9drd43ELV1YJAtucHiEg2e99syB+jaONt1ke2aq3ouTA77pEFxp944lpXhbJsdW8aW3ereTU8P024lEjgJdIm669syq4mx2rPazY3Ux2yoKSUuZ49Aw56CnyQv20KAfCL4vn5zujTIo3TKxmvev1sVQEnMU6BJVnHPUt/bxm9o2flPbxuunO7k6MkpWWjL3BnzsWF3M/auLKcm7ybaJc3DpLDSPhfzb3tfYPvnUTO+N1tCdNYUrNYlSopoCXaLa5cFhXmnoYM+pVvaebKW52/v0pjUlOexYU8yO1cVsWZZPSnIYgnZ0FDrqJ6/iL7wDw/3e8fRcL+R9AS/cC1dA0UrIX66+vEQFBbrEjLHRBHtOtbL3VCs1Zy8xPOrIyUhhe5Wf+6v83LfaT3FOGMN1ZBjaayev5DsbJmbKA2CQtxQKKydCvnCFF/oFFQp7WTAKdIlZPQNDvFzXzp5Trew51UZbcLDYxrI8dqz2c/+aYjaV59/4zpnrcaXTm0bZ2QCdp72vjuDP/Z0hJxrklkHRiomQLxz7uRJSrzE2QWSOFOgSF5xzHGvuYe+pVvaeauPQu5cYdVCQmcp9VX7uX13M9io/hVlp819M/6VgwJ+eCPux4L/SMfnc3LKQgA9Z3RdUQtocd/lIwlKgS1zqunKVfXXt7D3Zyt7aNjovX8UMNi/NZ8dqr/e+vjSXpPlYvc+kvysk5Kes7Me2Vo7JKQ2G/NTVfaUumJJrUqBL3BsddRxu6mZvsDVzuLEL58CXncZ9VcXsWOOnepWfvMwI70Xv7/Jmy3c0vLedE/rBIgA5SybCvXCl9z3T532KVEau9wZuRp62XiYYBboknPa+QfbVtrHnVBv7atvo7h8iOcnYuqyA+9d4+97XlORg0fTJSwPdk0M+tJ1zuXX6x6VlT4R7RvB7eu41fs6ffN7Yz2lZ+gSqGKJAl4Q2PDLKO41d7DnZxp5TreNDxUpyM7h/tdd7vzfgu7GZMwttoMfbU99/CQZ7vPAfCH4f7An+3BXyc/fEeaPDMz+3JUN6TshfCPnT/IUw9ee84Pcc718JbnSaLzfDsfk+xzE+0XM86270NjMcn+NzL14PZVtn/t9lGgp0kRAtPQP85pQX7vvr2ukbHCY12bitonA84Ff5sxe+9z4fnIOh/ilBP/Uvg+DtmX4mMjkRt+75LDz41Tk9VIEuMo2hkVEOnrsUvKipjVMt3lWk2ekprCnJYe2S3OBXDmtKclmUloD96tFR7+raaUO/G3DeoLQZvywM59zAczD2nZCWkoX59hwfm5YNi/Jv5H+FkJdToItcl6aufg4EZ72fuNDDyQu99A56LQszqCzKYm1pLuuCIb92SS4luRnR1YuXuDZToEdx01Bk4ZXlL+J3b1s2fts5R+Olfo5f6OF4MOQPN3bx4uEL4+fkZ6aytiSXdaUTq/lAcc7cB42JzJECXWQGZsbSwkyWFmbygfUl4/f3DAxx6mIvJ0KC/p9eP8fA0CgAKUnGquJs1i4ZW817QV+UnR6pX0USgAJdZA5yM1K5raKQ2yomPvd0ZNRxpv0yJy54AX/8Qg+vNLTzk7eaxs8pzkkf78uvK81l3ZIcKoqywjN4TBKeAl0kTJKDq/JVxdn89qbS8fs7L1+dCPnmiaAfGvHev0pPSWJ1SQ5rSyb68mtLc8nN0AdyyI3Rm6IiEXB1eJT61r5Jq/kTF3q4dGVo/JzygkUTq/klOaxbksfSwkV6AzbB6U1RkSiTlpLktVxKc8fvc87R0jM4KeCPX+jh1ydaGA2uu/IWpXJLeR4by/K4pTyfTUvztMtGxinQRaKEmVGSl0FJXgY71hSP399/dYRTLb0cb+7hSFMX75zv5n/tO81IMOX9OelsKs9jY1k+tyzNY1N5/sJMnJSoo0AXiXKL0pLZvDSfzUvzAW9L5cDQCMcv9HD4fBeHG7s53NTNr0+2jl9hXl6wiE3l+d5qPriiz1FPPu4p0EViUEZqMluWFbBlWcH4fb0DQxxt8vbJeyHfxYtHvP3yZrDClxUS8vmsL80lIzUBr3yNYwp0kTiRk5HKXSuLuGtl0fh9nZevTgR8Yxf769v5cXAbZUqSUbU4h01LvX78xrI8VpfkkKotlDFLu1xEEsjYG6/vNHaFBH033f3e7pr04Ju1Yyv5W8rzWOGLk0FlcUKzXERkWs453u28wjuN3eM9+aPN3Vy5OgJ4g8o2lI2FvBf05QXaPhkp2rYoItMyM5YXZbG8KItHghdEjYw6Gtr6eOf8RLvmH14+y9URb7RBYVYaG8vy2FTutWvWLMmhNG+RVvIRphW6iFyXq8OjnLrYO6ldU9vSO75HPjMtmVXF2QSKcwgsziYQ/Lm8QEEfTlqhi8hNS0tJ8rZAlucBywFvj/zxC92cuthHXWsv9a19HKhv44VDjeOPy0hNGg/6VcXZVC3OIVCczdLCTJIV9GF1XYFuZjuBbwLJwNPOua9NOb4M+AGQHzznSefcS2GuVUSizKK0ZLYuL2Tr8sJJ93f3D1Hf2kd9ay+1LX3Utfbx+umOSYPK0lKSWOnPpiq4ml9VnEPV4myWFWZqWNkczdpyMbNkoBZ4EGgE3gQedc4dDznnKeAt59x3zGwd8JJzrmKm51XLRSTx9A54QV/X2ud9b/ECv6mrf/yctOQkVvizJq3mA4uzWV6UpS2V3HzL5Xag3jl3Ovhk/wx8GDgeco4DxoZS5AHNcy9XROJVTkYqty4r4NaQC6IALg8O09DWF1zN91Lf0sfhxm5ePHJh/OrX1GSj0pc13roJLPYCv6IoSx8mEnQ9gV4GnA+53QjcMeWcPwd2m9mfAFnA+671RGb2OPA4wLJly651iogkoKz0lOCWyMmfs3nl6jCn2y5T19pLXYsX+Meau3np6ETQJycZFUWZBIItm1XBVX2lLyvhroQN15uijwLPOOf+h5ndBfxvM9vgnBsNPck59xTwFHgtlzC9tojEqcy0FDaU5bGhLG/S/QNDIzS0jbVtvFV9bUsvu49fHN91k2SwvCiLlf7s8Tn1geJsVhZnk50en/tBrue3agKWhtwuD94X6g+BnQDOuVfNLN7tXekAAAYZSURBVAPwAa3hKFJEJFRGajLrS/NYXzo56AeHRzjTftkL+ZZe6tu8wP9Nbev4B4oALMnLGA/5VcXZrPJnE1icE/NTKq8n0N8EAmZWiRfkvwd8Yso57wIPAM+Y2VogA2gLZ6EiIrNJT0lmTUkua0pyJ90/NDLKu51XgjtvJr7+z5vnx6+IBe+CqVV+bxUfCAn8JXmxMXN+1kB3zg2b2R8Dv8DbkrjLOXfMzP4CqHHO/RT4L8D3zOzP8N4g/Q8uUlcsiYhMkZrsbZFc6c/mA+sn7h8ddTR3948HfENwRf/zoxd4NuTTo7KCF02tnLKiX1qwKKq2WOpKURGRKZxzdFy+Or7FsiFkVX+xZ2D8vLTkJCp9WZPbN/P8hqyuFBURuQFmhi87HV92OneuKJp0rGdgaCLg2/qob+nj6JSdN0kGywozJ1b1wRX9Sn/WvH7QiAJdROQG5E6zl35gKPiG7Fj7Jvj9N7Vtk96QLcnN4FPVlXyqekXYa1Ogi4iEQUZqMmuX5LJ2yeQ3ZIdD35ANruj9OenzUoMCXURkHqUkJ7HCn80Kfzbvn+fXip63Z0VE5KYo0EVE4oQCXUQkTijQRUTihAJdRCROKNBFROKEAl1EJE4o0EVE4kTEhnOZWRtwbo4P9wHtYSwnXKK1Loje2lTXjVFdNyYe61runPNf60DEAv1mmFnNdNPGIila64LorU113RjVdWMSrS61XERE4oQCXUQkTsRqoD8V6QKmEa11QfTWprpujOq6MQlVV0z20EVE5L1idYUuIiJTKNBFROJEzAW6me00s1NmVm9mT0a6HgAz22VmrWZ2NNK1hDKzpWa2x8yOm9kxM/vTSNcEYGYZZvaGmb0TrOurka4plJklm9lbZvazSNcyxszOmtkRM3vbzKLm09XNLN/Mnjezk2Z2wszuioKaVgf/O4199ZjZZyNdF4CZ/Vnwz/xRM3vWzDLC+vyx1EM3s2SgFngQaATeBB51zh2PcF3bgT7gH51zGyJZSygzWwIscc4dMrMc4CDwkSj472VAlnOuz8xSgQPAnzrnXotkXWPM7D8D24Bc59yHIl0PeIEObHPORdVFMmb2A2C/c+5pM0sDMp1zXZGua0wwM5qAO5xzc72QMVy1lOH9WV/nnOs3s+eAl5xzz4TrNWJthX47UO+cO+2cuwr8M/DhCNeEc24f0BnpOqZyzl1wzh0K/twLnADKIlsVOE9f8GZq8CsqVhZmVg58EHg60rVEOzPLA7YD3wdwzl2NpjAPegBoiHSYh0gBFplZCpAJNIfzyWMt0MuA8yG3G4mCgIoFZlYB3Aq8HtlKPMG2xttAK/BL51xU1AV8A/ivwGikC5nCAbvN7KCZPR7pYoIqgTbgH4ItqqfNLCvSRU3xe8CzkS4CwDnXBPwd8C5wAeh2zu0O52vEWqDLHJhZNvAC8FnnXE+k6wFwzo045zYD5cDtZhbxVpWZfQhodc4djHQt13Cvc24L8BDwR8E2X6SlAFuA7zjnbgUuA1HxvhZAsAX0CPAvka4FwMwK8DoKlUApkGVmj4XzNWIt0JuApSG3y4P3yTSCPeoXgH9yzv040vVMFfwn+h5gZ6RrAe4BHgn2q/8Z+C0z+2FkS/IEV3c451qBn+C1HyOtEWgM+dfV83gBHy0eAg4551oiXUjQ+4Azzrk259wQ8GPg7nC+QKwF+ptAwMwqg3/7/h7w0wjXFLWCbz5+HzjhnPt6pOsZY2Z+M8sP/rwI703uk5GtCpxzX3DOlTvnKvD+bP2bcy6sK6i5MLOs4JvaBFsa7wcivqPKOXcROG9mq4N3PQBE9A33KR4lStotQe8Cd5pZZvD/mw/gva8VNinhfLL55pwbNrM/Bn4BJAO7nHPHIlwWZvYscD/gM7NG4CvOue9HtirAW3F+EjgS7FcD/Dfn3EsRrAlgCfCD4A6EJOA551zUbBGMQouBn3gZQArwI+fc/4tsSeP+BPin4ALrNPAHEa4HGP+L70HgM5GuZYxz7nUzex44BAwDbxHmEQAxtW1RRESmF2stFxERmYYCXUQkTijQRUTihAJdRCROKNBFROKEAl1EJE4o0EVE4sT/B+wlwwBDbHlEAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "User_Inputs =[\"the product was best product I would recommend it at all. I would even give ten stars.\",\n",
        "     \"it is horrible , not at all worth it. The product was defected and not working.\",\n",
        "     \"I received defected product.Return is also not possible. \\n The product shown in the image is very different from the one that I received\",\n",
        "     \"The product is complete worthless and I hate it. Definite two stars not more than that\",\n",
        "     \"I was skeptical about the product, it exceeded my expectation\",\n",
        "     \"Worst product ever made\",\n",
        "     \"The seller is fake , I recived the product late and it was also damaged. I am heavily dissapointed.\",\n",
        "     \"One word worth it. Every penny is worth it for this product\",\n",
        "     \"The product initally looked good but half the way the display went off. Very disappointing and misleading discription \",\n",
        "     \"The seller refused to refund the product even though it is defected.\"     ]\n",
        "\n",
        "for i in User_Inputs:\n",
        "  inp_review = clean(i,\"inputs\")\n",
        "  inp_review = ' '.join(inp_review)\n",
        "  inp_x = in_tokenizer.texts_to_sequences([inp_review]) \n",
        "  inp_x = pad_sequences(inp_x,  maxlen = max_in_len, padding='post')\n",
        "\n",
        "  summary = decode_sequence(inp_x.reshape(1,max_in_len))\n",
        "  if 'eos' in summary :\n",
        "    summary=summary.replace('eos','')\n",
        "  print(\"Actual Text:     \",i)\n",
        "  print(\"Predicted summary:\",summary);print(\"\\n\")"
      ],
      "metadata": {
        "id": "0GqCXWP42csP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e48b0b2-0cd7-4fcf-c220-53a86d6f2dc9"
      },
      "id": "0GqCXWP42csP",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Text:      the product was best product I would recommend it at all. I would even give ten stars.\n",
            "Predicted summary: great product  \n",
            "\n",
            "\n",
            "Actual Text:      it is horrible , not at all worth it. The product was defected and not working.\n",
            "Predicted summary: awful  \n",
            "\n",
            "\n",
            "Actual Text:      I received defected product.Return is also not possible. \n",
            " The product shown in the image is very different from the one that I received\n",
            "Predicted summary: poor quality  \n",
            "\n",
            "\n",
            "Actual Text:      The product is complete worthless and I hate it. Definite two stars not more than that\n",
            "Predicted summary: awful  \n",
            "\n",
            "\n",
            "Actual Text:      I was skeptical about the product, it exceeded my expectation\n",
            "Predicted summary: good  \n",
            "\n",
            "\n",
            "Actual Text:      Worst product ever made\n",
            "Predicted summary: horrible  \n",
            "\n",
            "\n",
            "Actual Text:      The seller is fake , I recived the product late and it was also damaged. I am heavily dissapointed.\n",
            "Predicted summary: misleading  \n",
            "\n",
            "\n",
            "Actual Text:      One word worth it. Every penny is worth it for this product\n",
            "Predicted summary: best  \n",
            "\n",
            "\n",
            "Actual Text:      The product initally looked good but half the way the display went off. Very disappointing and misleading discription \n",
            "Predicted summary: disappointed  \n",
            "\n",
            "\n",
            "Actual Text:      The seller refused to refund the product even though it is defected.\n",
            "Predicted summary: poor quality  \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "Custom_Approach_updated.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}