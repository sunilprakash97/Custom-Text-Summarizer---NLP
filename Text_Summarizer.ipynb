{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "-pJJrAFF6Ri7",
      "metadata": {
        "id": "-pJJrAFF6Ri7"
      },
      "source": [
        "## Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce04402",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ce04402",
        "outputId": "63004e2d-eb04-476e-9ce4-8bbece71a8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re                        \n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statistics import mode\n",
        "from bs4 import BeautifulSoup  \n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import LancasterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from pprint import pprint\n",
        "\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import backend as K \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,Attention\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SDlhfUe2ntKw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDlhfUe2ntKw",
        "outputId": "3b01a986-8e28-462f-f29e-effd0214c8bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count   Dtype \n",
            "---  ------                  --------------   ----- \n",
            " 0   Id                      100000 non-null  int64 \n",
            " 1   ProductId               100000 non-null  object\n",
            " 2   UserId                  100000 non-null  object\n",
            " 3   ProfileName             99996 non-null   object\n",
            " 4   HelpfulnessNumerator    100000 non-null  int64 \n",
            " 5   HelpfulnessDenominator  100000 non-null  int64 \n",
            " 6   Score                   100000 non-null  int64 \n",
            " 7   Time                    100000 non-null  int64 \n",
            " 8   Summary                 99998 non-null   object\n",
            " 9   Text                    100000 non-null  object\n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 7.6+ MB\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/Reviews.csv',nrows=100000)\n",
        "\n",
        "# data = pd.read_csv(\"Reviews.csv\",nrows=100000)\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results Before Preprocessing"
      ],
      "metadata": {
        "id": "22ENNiCKb7Qi"
      },
      "id": "22ENNiCKb7Qi"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text:   \")\n",
        "data['Text'][:1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "qvVeWpd-cAy2",
        "outputId": "42127407-2844-491b-8122-c675ca29d20a"
      },
      "id": "qvVeWpd-cAy2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:   \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary:  \")\n",
        "data['Summary'][:1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tw1eKGxMcCki",
        "outputId": "75f1250d-954b-4fb5-f066-ff37fbaa5714"
      },
      "id": "tw1eKGxMcCki",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Good Quality Dog Food'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bZOmVHMS6Y8R",
      "metadata": {
        "id": "bZOmVHMS6Y8R"
      },
      "source": [
        "## Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jbRH_OZBTNJW",
      "metadata": {
        "id": "jbRH_OZBTNJW"
      },
      "outputs": [],
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ulZYirla6eNz",
      "metadata": {
        "id": "ulZYirla6eNz"
      },
      "source": [
        "### Cleaning the Data from the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uO6OdJ0rTQYU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO6OdJ0rTQYU",
        "outputId": "563d1ed7-861d-4e86-c451-f7fcb7ad6086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ]
        }
      ],
      "source": [
        "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
        "data.dropna(axis = 0, inplace = True)\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    \n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping \n",
        "                          else t for t in newString.split(\" \")])    \n",
        "\n",
        "    if(num == 0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens = newString.split()\n",
        "\n",
        "    long_words = []\n",
        "    for i in tokens:\n",
        "        if len(i) > 3:                                           \n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t,0)) \n",
        "\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))\n",
        "\n",
        "data['cleaned_text'] = cleaned_text\n",
        "data['cleaned_summary'] = cleaned_summary\n",
        "\n",
        "data.replace('', np.nan, inplace = True)\n",
        "data.dropna(axis = 0, inplace = True)\n",
        "\n",
        "max_text_len = 30\n",
        "max_summary_len = 8\n",
        "\n",
        "cleaned_text = np.array(data['cleaned_text'])\n",
        "cleaned_summary = np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split()) <= max_summary_len and \n",
        "       len(cleaned_text[i].split()) <= max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "data = pd.DataFrame({'Text':short_text,'Summary':short_summary})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results of 1st Preprocessing"
      ],
      "metadata": {
        "id": "BWGocjvJbxZl"
      },
      "id": "BWGocjvJbxZl"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text :\")\n",
        "data['Text'][:1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "QFcRrZZ3bt-j",
        "outputId": "56dcb5d1-b0a6-46f0-87a9-6892a3b2bd20"
      },
      "id": "QFcRrZZ3bt-j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bought several vitality canned food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary\")\n",
        "data['Summary'][:1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3pkgl97Zb0yu",
        "outputId": "728e5fc8-1a4a-420e-fbcb-d158c383780f"
      },
      "id": "3pkgl97Zb0yu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good quality food'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data Observations\")\n",
        "print(\"Text:    \",len(data['Text']))\n",
        "print(\"Summary: \",len(data['Summary']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcaoho79b2Fx",
        "outputId": "7832f2e2-c209-426b-c8f9-4b03ba5327c3"
      },
      "id": "Gcaoho79b2Fx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Observations\n",
            "Text:     52559\n",
            "Summary:  52559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebJeVa446iIR",
      "metadata": {
        "id": "ebJeVa446iIR"
      },
      "source": [
        "### Finalizing the Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb39122c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb39122c",
        "outputId": "2080e2dc-26e2-4e15-8873-0ac0f04aa542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of input words :  16597\n",
            "number of target words :  9714\n"
          ]
        }
      ],
      "source": [
        "data.drop_duplicates(subset = ['Text'], inplace = True)\n",
        "data.dropna(axis = 0, inplace = True)\n",
        "input_data = data.loc[:,'Text']\n",
        "target_data = data.loc[:,'Summary']\n",
        "target_data.replace('', np.nan, inplace = True)\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_words = []\n",
        "target_words = []\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemm = LancasterStemmer()\n",
        "\n",
        "# This function is to clean the User's Input text\n",
        "def clean(texts,src):\n",
        "  words = word_tokenize(texts.lower())\n",
        "  words = list(filter(lambda w:(w.isalpha() and len(w) >= 3),words))\n",
        "  \n",
        "  # Stem the words to their root word and filter stop words\n",
        "  if src == \"inputs\":\n",
        "    words = [stemm.stem(w) for w in words if w not in stop_words]\n",
        "  else:\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "  return words\n",
        "\n",
        "for in_txt, tr_txt in zip(input_data, target_data):\n",
        "  in_words = clean(in_txt,\"inputs\")\n",
        "  input_texts += [' '.join(in_words)]\n",
        "  input_words += in_words\n",
        "\n",
        "  tr_words = clean(\"sos \"+ tr_txt +\" eos\", \"target\")\n",
        "  target_texts += [' '.join(tr_words)]\n",
        "  target_words += tr_words\n",
        "\n",
        "# Store only unique words\n",
        "input_words = sorted(list(set(input_words)))\n",
        "target_words = sorted(list(set(target_words)))\n",
        "num_in_words = len(input_words) \n",
        "num_tr_words = len(target_words)\n",
        "\n",
        "# Counting the Input and target texts which appears most often  \n",
        "max_in_len = mode([len(i) for i in input_texts])\n",
        "max_tr_len = mode([len(i) for i in target_texts])\n",
        "\n",
        "max_in_len = 30\n",
        "max_tr_len = 8\n",
        "print(\"number of input words : \",num_in_words)\n",
        "print(\"number of target words : \",num_tr_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results of 2nd Preprocessing"
      ],
      "metadata": {
        "id": "pOSDvWeAbaNh"
      },
      "id": "pOSDvWeAbaNh"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text :\")\n",
        "input_texts[:1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "b1g4Ojt4bfig",
        "outputId": "84ecd60f-0132-4251-c593-995f2a4b676c"
      },
      "id": "b1g4Ojt4bfig",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bought sev vit can food produc found good qual produc look lik stew process meat smel bet labrad finicky apprecy produc bet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary :\")\n",
        "target_texts[:1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bv7Fwo6Kbg0L",
        "outputId": "071f20df-4677-46c3-9208-2fcb06efde56"
      },
      "id": "bv7Fwo6Kbg0L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sos good quality food eos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data Observations\")\n",
        "print(\"Text:    \",len(input_texts))\n",
        "print(\"Summary: \",len(target_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcSW7ShTbiId",
        "outputId": "c9793109-492e-47c5-9fdb-40337874c24a"
      },
      "id": "qcSW7ShTbiId",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Observations\n",
            "Text:     52532\n",
            "Summary:  52532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BCroKrIr6pBC",
      "metadata": {
        "id": "BCroKrIr6pBC"
      },
      "source": [
        "## Data Segregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4a6af1",
      "metadata": {
        "id": "0f4a6af1"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input_texts,\n",
        "                                                    target_texts,\n",
        "                                                    test_size = 0.1,\n",
        "                                                    random_state = 1)\n",
        "\n",
        "# Tokenizing the training dataset\n",
        "in_tokenizer = Tokenizer()\n",
        "in_tokenizer.fit_on_texts(x_train)\n",
        "tr_tokenizer = Tokenizer()\n",
        "tr_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "# Converting text into sequence of integers\n",
        "x_train = in_tokenizer.texts_to_sequences(x_train) \n",
        "y_train = tr_tokenizer.texts_to_sequences(y_train) \n",
        "\n",
        "# Padding the array with 0's\n",
        "en_in_data = pad_sequences(x_train,  maxlen = max_in_len, padding='post') \n",
        "dec_data = pad_sequences(y_train,  maxlen = max_tr_len, padding='post')\n",
        "\n",
        "# Decoder input will excludes the last word i.e. 'eos'\n",
        "dec_in_data = dec_data[:,:-1]\n",
        "\n",
        "# Decoder target excludes the first word i.e 'sos'\n",
        "dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qfk4UPOl6_9x",
      "metadata": {
        "id": "Qfk4UPOl6_9x"
      },
      "source": [
        "## Building a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e2469d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7e2469d",
        "outputId": "a74a796a-081b-486c-9150-1aadcc46a782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 30, 100)      1659800     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 30, 300),    481200      ['embedding[0][0]']              \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 30, 300),    721200      ['lstm[0][0]']                   \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 100)    971500      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 30, 300),    721200      ['lstm_1[0][0]']                 \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 300),  481200      ['embedding_1[0][0]',            \n",
            "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " attention (Attention)          (None, None, 300)    0           ['lstm_3[0][0]',                 \n",
            "                                                                  'lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            " concat_layer1 (Concatenate)    (None, None, 600)    0           ['lstm_3[0][0]',                 \n",
            "                                                                  'attention[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 9715)   5838715     ['concat_layer1[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,874,815\n",
            "Trainable params: 10,874,815\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "K.clear_session() \n",
        "latent_dim = 300\n",
        "emb_dim = 100\n",
        "en_inputs = Input(shape = (max_in_len,)) \n",
        "en_embedding = Embedding(num_in_words+1,\n",
        "                         emb_dim,\n",
        "                         trainable = True)(en_inputs) \n",
        "\n",
        "# LSTM 1\n",
        "en_lstm1 = LSTM(latent_dim, \n",
        "                return_state = True, \n",
        "                return_sequences = True, \n",
        "                dropout = 0.4, \n",
        "                recurrent_dropout = 0.4) \n",
        "en_outputs1, state_h1, state_c1 = en_lstm1(en_embedding) \n",
        "\n",
        "# LSTM2\n",
        "en_lstm2 = LSTM(latent_dim,\n",
        "                return_state = True,\n",
        "                return_sequences = True, \n",
        "                dropout = 0.4, \n",
        "                recurrent_dropout = 0.4)\n",
        "en_outputs2, state_h2, state_c2 = en_lstm2(en_outputs1) \n",
        "\n",
        "# LSTM3\n",
        "en_lstm3 = LSTM(latent_dim, \n",
        "                return_state = True, \n",
        "                return_sequences = True, \n",
        "                dropout = 0.4, \n",
        "                recurrent_dropout = 0.4)\n",
        "en_outputs3 , state_h3, state_c3 = en_lstm3(en_outputs2)\n",
        "en_states = [state_h3, state_c3]\n",
        "\n",
        "# Decoder \n",
        "dec_inputs = Input(shape = (None, )) \n",
        "dec_emb_layer = Embedding(num_tr_words+1, \n",
        "                          emb_dim,\n",
        "                          trainable = True) \n",
        "dec_embedding = dec_emb_layer(dec_inputs) \n",
        "\n",
        "# Initialize decoder's LSTM\n",
        "dec_lstm = LSTM(latent_dim, \n",
        "                return_sequences = True, \n",
        "                return_state = True,\n",
        "                dropout = 0.4, \n",
        "                recurrent_dropout = 0.4)\n",
        "dec_outputs, *_ = dec_lstm(dec_embedding, \n",
        "                           initial_state = en_states) \n",
        "\n",
        "# Attention layer\n",
        "attention = Attention()\n",
        "attn_out = attention([dec_outputs, en_outputs3])\n",
        "\n",
        "# Concatenate the attention output with the decoder ouputs\n",
        "merge = Concatenate(axis = -1, name = 'concat_layer1')([dec_outputs, \n",
        "                                                        attn_out])\n",
        "\n",
        "# Dense layer\n",
        "dec_dense = Dense(num_tr_words+1, activation = 'softmax') \n",
        "dec_outputs = dec_dense(merge) \n",
        "\n",
        "# Model Summary\n",
        "model = Model([en_inputs, dec_inputs], dec_outputs) \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "ZWiTRjfjeB1f"
      },
      "id": "ZWiTRjfjeB1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "390e14e7",
      "metadata": {
        "id": "390e14e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e739f93f-0f12-4329-c982-84054ac6932c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "333/333 [==============================] - 186s 521ms/step - loss: 2.5724 - accuracy: 0.6623 - val_loss: 2.3703 - val_accuracy: 0.6864\n",
            "Epoch 2/50\n",
            "333/333 [==============================] - 158s 475ms/step - loss: 2.3216 - accuracy: 0.6863 - val_loss: 2.3060 - val_accuracy: 0.6876\n",
            "Epoch 3/50\n",
            "333/333 [==============================] - 158s 475ms/step - loss: 2.2468 - accuracy: 0.6884 - val_loss: 2.2464 - val_accuracy: 0.6888\n",
            "Epoch 4/50\n",
            "333/333 [==============================] - 156s 469ms/step - loss: 2.1968 - accuracy: 0.6894 - val_loss: 2.2260 - val_accuracy: 0.6897\n",
            "Epoch 5/50\n",
            "333/333 [==============================] - 157s 472ms/step - loss: 2.1604 - accuracy: 0.6913 - val_loss: 2.2043 - val_accuracy: 0.6912\n",
            "Epoch 6/50\n",
            "333/333 [==============================] - 157s 472ms/step - loss: 2.1251 - accuracy: 0.6933 - val_loss: 2.1923 - val_accuracy: 0.6923\n",
            "Epoch 7/50\n",
            "333/333 [==============================] - 154s 464ms/step - loss: 2.0904 - accuracy: 0.6951 - val_loss: 2.1604 - val_accuracy: 0.6946\n",
            "Epoch 8/50\n",
            "333/333 [==============================] - 157s 472ms/step - loss: 2.0584 - accuracy: 0.6966 - val_loss: 2.1462 - val_accuracy: 0.6946\n",
            "Epoch 9/50\n",
            "333/333 [==============================] - 158s 473ms/step - loss: 2.0309 - accuracy: 0.6986 - val_loss: 2.1321 - val_accuracy: 0.6964\n",
            "Epoch 10/50\n",
            "333/333 [==============================] - 159s 477ms/step - loss: 2.0041 - accuracy: 0.7000 - val_loss: 2.1299 - val_accuracy: 0.6975\n",
            "Epoch 11/50\n",
            "333/333 [==============================] - 155s 466ms/step - loss: 1.9804 - accuracy: 0.7020 - val_loss: 2.1221 - val_accuracy: 0.6965\n",
            "Epoch 12/50\n",
            "333/333 [==============================] - 155s 467ms/step - loss: 1.9571 - accuracy: 0.7038 - val_loss: 2.1118 - val_accuracy: 0.6989\n",
            "Epoch 13/50\n",
            "333/333 [==============================] - 154s 461ms/step - loss: 1.9333 - accuracy: 0.7053 - val_loss: 2.1101 - val_accuracy: 0.6968\n",
            "Epoch 14/50\n",
            "333/333 [==============================] - 156s 467ms/step - loss: 1.9141 - accuracy: 0.7064 - val_loss: 2.1092 - val_accuracy: 0.6991\n",
            "Epoch 15/50\n",
            "333/333 [==============================] - 153s 461ms/step - loss: 1.8986 - accuracy: 0.7074 - val_loss: 2.1099 - val_accuracy: 0.6985\n",
            "Epoch 16/50\n",
            "333/333 [==============================] - 154s 464ms/step - loss: 1.8822 - accuracy: 0.7088 - val_loss: 2.1174 - val_accuracy: 0.6983\n",
            "Epoch 16: early stopping\n",
            "INFO:tensorflow:Assets written to: Model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1011b03fd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1011b3a590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f10105e7150> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1010646890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "plot_model(model, \n",
        "           to_file ='model_plot.png', \n",
        "           show_shapes = True, \n",
        "           show_layer_names = True)\n",
        "\n",
        "model.compile(optimizer = \"rmsprop\", \n",
        "              loss = \"sparse_categorical_crossentropy\", \n",
        "              metrics = [\"accuracy\"] ) \n",
        "\n",
        "es = EarlyStopping(monitor = 'val_loss', \n",
        "                   mode ='min', \n",
        "                   verbose = 1, \n",
        "                   patience = 2)\n",
        "\n",
        "graph = model.fit([en_in_data, dec_in_data],\n",
        "                    dec_tr_data, \n",
        "                    batch_size = 128, \n",
        "                    epochs = 50,\n",
        "                    callbacks = [es], \n",
        "                    validation_split = 0.1)\n",
        "\n",
        "model.save(\"Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Performance"
      ],
      "metadata": {
        "id": "ucWS1ekSaBLH"
      },
      "id": "ucWS1ekSaBLH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WeyiXhU-C1CE",
      "metadata": {
        "id": "WeyiXhU-C1CE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "befa5676-a2d2-40d1-fd6a-739e9b4e1297"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(graph.history['loss'], label='train')\n",
        "pyplot.plot(graph.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Inference"
      ],
      "metadata": {
        "id": "VZRXLL1gaJqs"
      },
      "id": "VZRXLL1gaJqs"
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.load_model(\"Model\")\n",
        "\n",
        "# Encoder Inference\n",
        "latent_dim = 300\n",
        "\n",
        "## Encoder Model\n",
        "en_outputs, state_h_enc, state_c_enc = model.layers[6].output\n",
        "en_states = [state_h_enc, state_c_enc]\n",
        "en_model = Model(model.input[0], [en_outputs] + en_states)\n",
        "\n",
        "# Decoder Inference\n",
        "dec_state_input_h = Input(shape = (latent_dim, ))\n",
        "dec_state_input_c = Input(shape = (latent_dim, ))\n",
        "dec_hidden_state_input = Input(shape = (max_in_len, latent_dim))\n",
        "\n",
        "dec_inputs = model.input[1]\n",
        "dec_emb_layer = model.layers[5]\n",
        "dec_lstm = model.layers[7]\n",
        "dec_embedding = dec_emb_layer(dec_inputs)\n",
        "\n",
        "# Initialize Decoder LSTM\n",
        "dec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, \n",
        "                                            initial_state = [dec_state_input_h,\n",
        "                                                             dec_state_input_c])\n",
        "\n",
        "# Attention layer\n",
        "attention = model.layers[8]\n",
        "attn_out2 = attention([dec_outputs2, dec_hidden_state_input])\n",
        "\n",
        "# Concatenating Layer\n",
        "merge2 = Concatenate(axis = -1)([dec_outputs2, attn_out2])\n",
        "\n",
        "# Dense layer\n",
        "dec_dense = model.layers[10]\n",
        "dec_outputs2 = dec_dense(merge2)\n",
        "\n",
        "## Decoder Model\n",
        "dec_model = Model(\n",
        "[dec_inputs] + [dec_hidden_state_input, dec_state_input_h, dec_state_input_c],\n",
        "[dec_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "p1GCBfU5ase8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379e37b3-2780-49d5-ce25-7d55f0817e99"
      },
      "id": "p1GCBfU5ase8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding Sequence"
      ],
      "metadata": {
        "id": "NVw0v6SGaT9Y"
      },
      "id": "NVw0v6SGaT9Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverting Back to words from tokenizer\n",
        "reverse_target_word_index = tr_tokenizer.index_word\n",
        "reverse_source_word_index = in_tokenizer.index_word\n",
        "target_word_index = tr_tokenizer.word_index\n",
        "reverse_target_word_index[0] = ' '\n",
        "\n",
        "def decode_sequence(input):\n",
        "    encoder_out, encoder_h, encoder_c = en_model.predict(input)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_word_index['sos']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    \n",
        "    while not stop_condition: \n",
        "        output_words, decoder_h, decoder_c = dec_model.predict([target_seq] + [encoder_out,\n",
        "                                                                                   encoder_h,  \n",
        "                                                                                   encoder_c])\n",
        "        word_index = np.argmax(output_words[0, -1, :])\n",
        "        text_word = reverse_target_word_index[word_index]\n",
        "        decoded_sentence += text_word +\" \"\n",
        "\n",
        "        if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n",
        "          stop_condition = True\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = word_index\n",
        "        encoder_h, encoder_c = decoder_h, decoder_c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "3f1a9McnahIG"
      },
      "id": "3f1a9McnahIG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "UeGuNepvaX5_"
      },
      "id": "UeGuNepvaX5_"
    },
    {
      "cell_type": "code",
      "source": [
        "User_Inputs =[\"the product was best product I would recommend it at all. I would even give ten stars.\",\n",
        "     \"it is horrible , not at all worth it. The product was defected and not working.\",\n",
        "     \"I received defected product.Return is also not possible. \\n The product shown in the image is very different from the one that I received\",\n",
        "     \"The product is complete worthless and I hate it. Definite two stars not more than that\",\n",
        "     \"I was skeptical about the product, it exceeded my expectation\",\n",
        "     \"Worst product ever made\",\n",
        "     \"The seller is fake , I recived the product late and it was also damaged. I am heavily dissapointed.\",\n",
        "     \"One word worth it. Every penny is worth it for this product\",\n",
        "     \"The product initally looked good but half the way the display went off. Very disappointing and misleading discription \",\n",
        "     \"The seller refused to refund the product even though it is defected.\"     ]\n",
        "\n",
        "for i in User_Inputs:\n",
        "  inp_review = clean(i,\"inputs\")\n",
        "  inp_review = ' '.join(inp_review)\n",
        "  inp_x = in_tokenizer.texts_to_sequences([inp_review]) \n",
        "  inp_x = pad_sequences(inp_x,  maxlen = max_in_len, padding = 'post')\n",
        "\n",
        "  summary = decode_sequence(inp_x.reshape(1, max_in_len))\n",
        "  if 'eos' in summary :\n",
        "    summary = summary.replace('eos','')\n",
        "  print(\"Actual Text:     \",i)\n",
        "  print(\"Predicted summary:\",summary);\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTBTZSxEjH0p",
        "outputId": "aadb761b-5133-4878-cbde-6fa6005e0c77"
      },
      "id": "RTBTZSxEjH0p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Text:      the product was best product I would recommend it at all. I would even give ten stars.\n",
            "Predicted summary: great product \n",
            "\n",
            "\n",
            "Actual Text:      it is horrible , not at all worth it. The product was defected and not working.\n",
            "Predicted summary: horrible \n",
            "\n",
            "\n",
            "Actual Text:      I received defected product.Return is also not possible. \n",
            " The product shown in the image is very different from the one that I received\n",
            "Predicted summary: damaged cans \n",
            "\n",
            "\n",
            "Actual Text:      The product is complete worthless and I hate it. Definite two stars not more than that\n",
            "Predicted summary: good  \n",
            "\n",
            "\n",
            "Actual Text:      I was skeptical about the product, it exceeded my expectation\n",
            "Predicted summary: good  \n",
            "\n",
            "\n",
            "Actual Text:      Worst product ever made\n",
            "Predicted summary: horrible \n",
            "\n",
            "\n",
            "Actual Text:      The seller is fake , I recived the product late and it was also damaged. I am heavily dissapointed.\n",
            "Predicted summary: beware  \n",
            "\n",
            "\n",
            "Actual Text:      One word worth it. Every penny is worth it for this product\n",
            "Predicted summary: great  \n",
            "\n",
            "\n",
            "Actual Text:      The product initally looked good but half the way the display went off. Very disappointing and misleading discription \n",
            "Predicted summary: awful  \n",
            "\n",
            "\n",
            "Actual Text:      The seller refused to refund the product even though it is defected.\n",
            "Predicted summary: misleading \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text_Summarizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}